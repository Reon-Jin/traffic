import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torch.nn import functional as F
import os, sys
import numpy as np
from sklearn.metrics import average_precision_score
from torch.utils.data import random_split
from torch.optim.lr_scheduler import StepLR
import numpy as np
from sklearn.metrics import average_precision_score
import numpy as np
import torch
from sklearn.metrics import precision_score, recall_score  # æ–°å¢å¯¼å…¥
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, average_precision_score

# ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼Œæ·»åŠ é¡¹ç›®ç›®å½•ï¼Œæ–¹ä¾¿ä¸åŒè·¯å¾„çš„è„šæœ¬æ¨¡å—è°ƒç”¨
curPath = os.path.abspath(os.path.dirname(__file__))
sys.path.append(curPath)

from Mymodel.Vit import ST_ViT
from Mymodel import dataloader

import logging
import datetime
import torch

torch.autograd.set_detect_anomaly(True)


def log_init():
    log_dir = "newlog"
    os.makedirs(log_dir, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰

    # é…ç½®æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(levelname)s - %(message)s'  # ä»…ä¿ç•™æ—¥å¿—çº§åˆ«å’Œæ¶ˆæ¯
    )
    # formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"training_{timestamp}.log"
    file_handler = logging.FileHandler(os.path.join(log_dir, log_filename))
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    # æ§åˆ¶å°è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)


log_init()


def evaluate_single_step(pred, true, L=None):
    """
    å•æ—¶é—´æ­¥è¯„ä¼°å‡½æ•°ï¼ˆbatch_size, num_nodesï¼‰

    å‚æ•°:
        pred: é¢„æµ‹å¼ é‡ (batch_size, num_nodes)
        true: çœŸå®å€¼å¼ é‡ (batch_size, num_nodes)
        L: ç”¨äº Acc@L çš„é«˜é£é™©åŒºåŸŸå¤§å°ï¼ŒNone æ—¶ä»…è®¡ç®— RMSE å’Œ MAP

    è¿”å›:
        metrics: åŒ…å« RMSE, Acc@L, MAP çš„å­—å…¸
    """
    device = pred.device
    batch_size, num_nodes = pred.shape

    # è½¬æ¢ä¸º numpy è¿›è¡Œè®¡ç®—
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()

    # åˆå§‹åŒ–ç»“æœå­—å…¸
    metrics = {}

    # è®¡ç®— RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰
    rmse_error = np.mean((pred_np - true_np) ** 2)
    metrics['RMSE'] = np.sqrt(rmse_error)

    # è®¡ç®—Acc@Lï¼ˆä»…å…³æ³¨é«˜é£é™©åŒºåŸŸTop - Lï¼‰
    if L is not None:
        acc_list = []
        for b in range(batch_size):
            # è·å–é¢„æµ‹å’ŒçœŸå®çš„top Lç´¢å¼•
            pred_top = np.argsort(-pred_np[b])[:L]
            print(pred_top)
            true_top = np.argsort(-true_np[b])[:L]
            print(true_top)
            intersection = np.intersect1d(pred_top, true_top)
            print(intersection)
            sys.exit()
            acc_list.append(len(intersection) / L)
        metrics[f'Acc@{L}'] = np.mean(acc_list)

    # è®¡ç®— MAPï¼ˆä¸å†é™åˆ¶å‰ L ä¸ªé«˜é£é™©åŒºåŸŸï¼Œè€Œæ˜¯è®¡ç®—æ•´ä½“ MAPï¼‰
    map_list = []
    for b in range(batch_size):
        y_true = true_np[b]  # çœŸå®å€¼
        y_score = pred_np[b]  # é¢„æµ‹å€¼

        # è®¡ç®— APï¼ˆå¦‚æœ y_true åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼ŒAP è®¡ç®—ä¼šå‡ºé”™ï¼Œæ‰€ä»¥åšæ£€æŸ¥ï¼‰
        if np.unique(y_true).size > 1:
            ap = average_precision_score(y_true, y_score)
        else:
            ap = 0.0  # é¿å…è®¡ç®—å‡ºé”™ï¼ŒAP è®¾ä¸º 0

        map_list.append(ap)

    metrics['MAP'] = np.mean(map_list)

    return metrics


def calculate_metrics(pred, true):
    """
    è®¡ç®—äºŒåˆ†ç±»ä»»åŠ¡çš„ F1 Score, Accuracy, AUC-PR, AUC-ROC, Precision, Recall

    å‚æ•°:
        pred (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„é¢„æµ‹æ¦‚ç‡ï¼ˆå½¢çŠ¶ä»»æ„ï¼Œä¼šè¢«å±•å¹³ï¼‰
        true (torch.Tensor): çœŸå®æ ‡ç­¾ï¼ˆ0æˆ–1ï¼Œå½¢çŠ¶ä»»æ„ï¼Œä¼šè¢«å±•å¹³ï¼‰

    è¿”å›:
        dict: åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
    """
    # è½¬æ¢å¼ é‡åˆ°numpyå¹¶è„±ç¦»è®¡ç®—å›¾
    pred_np = pred.detach().cpu().numpy().flatten()
    true_np = true.detach().cpu().numpy().flatten()

    # åˆå§‹åŒ–æŒ‡æ ‡å­—å…¸
    metrics = {}

    # å¤„ç†AUC-ROCå¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼ˆå¦‚å•ä¸€ç±»åˆ«ï¼‰
    try:
        metrics["AUC-ROC"] = roc_auc_score(true_np, pred_np)
    except ValueError:
        metrics["AUC-ROC"] = float('nan')

    # è®¡ç®—AUC-PRï¼ˆæ­¤æŒ‡æ ‡åœ¨æ— æ­£æ ·æœ¬æ—¶è‡ªåŠ¨è¿”å›0ï¼‰
    metrics["AUC-PR"] = average_precision_score(true_np, pred_np)

    # äºŒå€¼åŒ–é¢„æµ‹å€¼å¹¶è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    pred_binary = (pred_np >= 0.5).astype(np.int32)

    # æ·»åŠ  Precision å’Œ Recallï¼ˆå¤„ç† zero_division è­¦å‘Šï¼‰
    metrics["Accuracy"] = accuracy_score(true_np, pred_binary)
    metrics["Precision"] = precision_score(true_np, pred_binary, zero_division=0)  # æ–°å¢
    metrics["Recall"] = recall_score(true_np, pred_binary, zero_division=0)  # æ–°å¢
    metrics["F1 Score"] = f1_score(true_np, pred_binary, zero_division=0)  # ä¿®å¤åŸæœ‰ä»£ç çš„æ½œåœ¨è­¦å‘Š

    return metrics


def calculate_rmse(pred, true):
    """
    è®¡ç®—å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰
    """
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()
    rmse_error = np.mean((pred_np - true_np) ** 2)
    return np.sqrt(rmse_error)


def calculate_acc_at_L(pred, true, L):
    """
    è®¡ç®—Acc@LæŒ‡æ ‡
    """
    batch_size, num_nodes = pred.shape
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()

    acc_list = []
    for b in range(batch_size):
        # è·å–é¢„æµ‹å’ŒçœŸå®çš„top Lç´¢å¼•
        pred_top = np.argsort(-pred_np[b])[:L]
        true_top = np.argsort(-true_np[b])[:L]
        intersection = np.intersect1d(pred_top, true_top)
        acc_list.append(len(intersection) / L)
    return np.mean(acc_list)


def calculate_acc_at_dynamic_L(pred, true):
    """
    è®¡ç®—åŠ¨æ€Acc@LæŒ‡æ ‡ï¼ŒLä¸ºæ¯ä¸ªæ ·æœ¬çš„çœŸå®æ­£æ ‡ç­¾æ•°é‡
    """
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()
    batch_size, num_nodes = pred_np.shape

    acc_list = []
    for b in range(batch_size):
        # è·å–å½“å‰æ ·æœ¬çš„çœŸå®æ­£æ ‡ç­¾æ•°é‡
        true_labels = true_np[b]
        L = int(np.sum(true_labels))  # çœŸå®æ­£æ ·æœ¬æ•°ä½œä¸ºL

        # å¤„ç†æ— æ­£æ ·æœ¬çš„æƒ…å†µï¼ˆåˆ†æ¯ä¸º0æ—¶è·³è¿‡ï¼‰
        if L == 0:
            acc_list.append(0.0)  # æˆ–æ ¹æ®éœ€æ±‚æ”¹ä¸ºè·³è¿‡ continue
            continue

        # è·å–é¢„æµ‹çš„Top-Lç´¢å¼•ï¼ˆæ ¹æ®é¢„æµ‹æ¦‚ç‡æ’åºï¼‰
        pred_top = np.argsort(-pred_np[b])[:L]

        # è·å–çœŸå®çš„Top-Lç´¢å¼•ï¼ˆç›´æ¥å–æ‰€æœ‰æ­£æ ·æœ¬ï¼‰
        true_top = np.where(true_labels == 1)[0]  # ç­‰ä»·äºçœŸå®Top-L

        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        intersection = np.intersect1d(pred_top, true_top)
        acc = len(intersection) / L
        acc_list.append(acc)

    return np.mean(acc_list) if acc_list else 0.0  # ç©ºåˆ—è¡¨è¿”å›0


def calculate_dynamic_map(pred, true):
    """
    åŠ¨æ€è°ƒæ•´Lå€¼çš„MAPè®¡ç®—å‡½æ•°

    å‚æ•°ï¼š
    pred : torch.Tensor - æ¨¡å‹é¢„æµ‹åˆ†æ•° [batch_size, num_nodes]
    true : torch.Tensor - çœŸå®æ ‡ç­¾ï¼ˆ0/1ï¼‰[batch_size, num_nodes]

    è¿”å›ï¼š
    float - æœ‰æ•ˆæ ·æœ¬çš„å¹³å‡MAPå€¼
    """
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()
    batch_size, num_nodes = pred_np.shape

    map_list = []

    for b in range(batch_size):
        # è·å–å½“å‰æ ·æœ¬æ•°æ®
        pred_scores = pred_np[b]
        true_labels = true_np[b]

        # è®¡ç®—åŠ¨æ€Lå€¼ï¼ˆçœŸå®äº‹æ•…æ•°é‡ï¼‰
        L = int(np.sum(true_labels))
        if L == 0:
            continue  # è·³è¿‡æ— äº‹æ•…æ ·æœ¬

        # è·å–é¢„æµ‹çš„Top LåŒºåŸŸ
        pred_top = np.argsort(-pred_scores)[:L]  # é™åºæ’åˆ—å–å‰Lä¸ª

        # è·å–çœŸå®äº‹æ•…åŒºåŸŸ
        true_positives = np.where(true_labels == 1)[0]
        true_set = set(true_positives)

        # è®¡ç®—å¹³å‡ç²¾åº¦
        cumulative_correct = 0
        precision_sum = 0.0

        for rank, node in enumerate(pred_top, 1):
            # è®¡ç®—å½“å‰èŠ‚ç‚¹æ˜¯å¦å‘½ä¸­
            hit = 1 if node in true_set else 0

            # æ›´æ–°ç´¯è®¡æ­£ç¡®æ•°
            cumulative_correct += hit

            # è®¡ç®—å½“å‰ç²¾åº¦
            current_precision = cumulative_correct / rank

            # ç´¯åŠ åŠ æƒç²¾åº¦
            precision_sum += current_precision * hit

        # è®¡ç®—å•ä¸ªæ ·æœ¬çš„AP
        ap = precision_sum / L if L > 0 else 0.0
        map_list.append(ap)

    # å¤„ç†å…¨batchæ— äº‹æ•…çš„ç‰¹æ®Šæƒ…å†µ
    if not map_list:
        print("Warning: All samples have zero true accidents")
        return 0.0

    return np.mean(map_list)


def calculate_map(pred, true, L):
    """
    è®¡ç®—Mean Average Precision (MAP@L)

    å‚æ•°ï¼š
    pred : torch.Tensor - æ¨¡å‹é¢„æµ‹åˆ†æ•° [batch_size, num_nodes]
    true : torch.Tensor - çœŸå®åˆ†æ•° [batch_size, num_nodes]
    L : int - éœ€è¦è¯„ä¼°çš„Top Læ’åé•¿åº¦

    è¿”å›ï¼š
    float - æ•´ä¸ªbatchçš„å¹³å‡MAPå€¼
    """
    # è½¬æ¢ä¸ºCPU numpyæ•°ç»„
    pred_np = pred.cpu().numpy()
    true_np = true.cpu().numpy()
    batch_size, num_nodes = pred_np.shape

    # ç»“æœå­˜å‚¨åˆ—è¡¨
    map_list = []

    for b in range(batch_size):
        # è·å–å½“å‰æ ·æœ¬çš„é¢„æµ‹å’ŒçœŸå®å€¼
        pred_scores = pred_np[b]
        true_scores = true_np[b]

        # ç”Ÿæˆé¢„æµ‹å’ŒçœŸå®çš„Top Lç´¢å¼•
        pred_topL = np.argsort(-pred_scores)[:L]  # é™åºæ’åˆ—å–å‰L
        true_topL = np.argsort(-true_scores)[:L]
        true_set = set(true_topL)

        # åˆå§‹åŒ–ç´¯è®¡å˜é‡
        cumulative_correct = 0
        sum_precision = 0.0

        for position in range(L):
            current_node = pred_topL[position]

            # è®¡ç®—real(j)
            real = 1 if current_node in true_set else 0

            # æ›´æ–°ç´¯è®¡æ­£ç¡®æ•°
            cumulative_correct += real

            # è®¡ç®—pre(j) = æ­£ç¡®æ•° / å½“å‰ä½ç½®(ä»1å¼€å§‹)
            denominator = position + 1  # é¿å…é™¤é›¶é”™è¯¯
            precision_at_j = cumulative_correct / denominator if denominator != 0 else 0.0

            # ç´¯åŠ ç²¾åº¦è´¡çŒ®
            sum_precision += precision_at_j * real

        # è®¡ç®—å½“å‰æ ·æœ¬çš„AP
        ap = sum_precision / L
        map_list.append(ap)

    # è¿”å›æ•´ä¸ªbatchçš„å¹³å‡MAP
    return np.mean(map_list)


# åˆ›å»ºæ•°æ®é›†å¯¹è±¡
class TimeSeriesDataset(Dataset):
    def __init__(self, grid_X_c, grid_y_c, grid_X_f, grid_y_f, node_X_c, node_X_f, target_time_feature):
        self.X_c = torch.tensor(grid_X_c, dtype=torch.float32)
        self.y_c = torch.tensor(grid_y_c, dtype=torch.float32)
        self.X_f = torch.tensor(grid_X_f, dtype=torch.float32)
        self.y_f = torch.tensor(grid_y_f, dtype=torch.float32)
        self.node_X_c = torch.tensor(node_X_c, dtype=torch.float32)
        self.node_X_f = torch.tensor(node_X_f, dtype=torch.float32)
        self.target_time_feature = torch.tensor(target_time_feature, dtype=torch.float32)
        # self.flag_c = torch.tensor(flag_c, dtype=torch.float32)
        # self.flag_f = torch.tensor(flag_f, dtype=torch.float32)

    def __len__(self):
        return len(self.X_c)

    def __getitem__(self, idx):
        return self.X_c[idx], self.y_c[idx], self.X_f[idx], self.y_f[idx], self.node_X_c[idx], self.node_X_f[idx], \
        self.target_time_feature[idx]


# åŠ è½½æ•°æ®
data_path = 'D:\python_object\æ·±åº¦å­¦ä¹ å¤§ä½œä¸š\æœª251121å¤§ä½œä¸š\homework\preprocessed data\CHI'
# æ—¶ç©ºæ•°æ®ï¼šï¼ˆT,D,H,Wï¼‰
all_data_c_path = os.path.join(data_path, 'new_grid_data_c_4d.npy')
all_data_f_path = os.path.join(data_path, 'new_grid_data_f_4d.npy')

# æ˜ å°„çŸ©é˜µï¼šï¼ˆHW,validï¼‰
grid_node_map_c_path = os.path.join(data_path, 'new_grid_node_map_c.npy')
grid_node_map_f_path = os.path.join(data_path, 'new_grid_node_map_f.npy')
# é™æ€ç‰¹å¾
static_feat_c_path = os.path.join(data_path, 'new_static_feat_c.npy')
static_feat_f_path = os.path.join(data_path, 'new_static_feat_f.npy')

# è¯­ä¹‰è¾¹
poi_adj_c_path = os.path.join(data_path, 'new_poi_adj_matrix_c.npy')
poi_adj_f_path = os.path.join(data_path, 'new_poi_adj_matrix_f.npy')
risk_adj_c_path = os.path.join(data_path, 'new_risk_adj_matrix_c.npy')
risk_adj_f_path = os.path.join(data_path, 'new_risk_adj_matrix_f.npy')
road_adj_c_path = os.path.join(data_path, 'new_road_adj_matrix_c.npy')
road_adj_f_path = os.path.join(data_path, 'new_road_adj_matrix_f.npy')

# æ˜ å°„çŸ©é˜µï¼šï¼ˆHW,validï¼‰
grid_node_map_c = np.load(grid_node_map_c_path)
grid_node_map_f = np.load(grid_node_map_f_path)
# é™æ€ç‰¹å¾
static_feat_c = np.load(static_feat_c_path)
static_feat_f = np.load(static_feat_f_path)

# è¯­ä¹‰è¾¹
poi_adj_c = np.load(poi_adj_c_path)
poi_adj_f = np.load(poi_adj_f_path)
risk_adj_c = np.load(risk_adj_c_path)
risk_adj_f = np.load(risk_adj_f_path)
road_adj_c = np.load(road_adj_c_path)
road_adj_f = np.load(road_adj_f_path)

# æ„é€ æ•°æ®é›†
X_c, y_c, node_X_c, target_time_features = dataloader.dataset_generate(all_data_c_path, grid_node_map_c)
X_f, y_f, node_X_f, _ = dataloader.dataset_generate(all_data_f_path, grid_node_map_f)

dataset = TimeSeriesDataset(X_c, y_c, X_f, y_f, node_X_c, node_X_f, target_time_features)

# è®¾ç½®è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ¯”ä¾‹ä¸º 6:2:2
train_ratio = 0.6
val_ratio = 0.2
test_ratio = 0.2

train_size = int(train_ratio * len(dataset))
val_size = int(val_ratio * len(dataset))
test_size = len(dataset) - train_size - val_size

# åˆ’åˆ†æ•°æ®é›†
train_dataset, val_dataset, test_dataset = random_split(
    dataset,
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)
)

# è¶…å‚æ•°è®¾ç½®
batch_size = 8
num_workers = 0
learning_rate = 0.0005
num_epochs = 200

# åˆ›å»ºDataLoader
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# é…ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å°†å¸¸é‡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
grid_node_map_c = torch.from_numpy(grid_node_map_c).to(device, dtype=torch.float32)
grid_node_map_f = torch.from_numpy(grid_node_map_f).to(device, dtype=torch.float32)

static_feat_c = torch.from_numpy(static_feat_c).to(device, dtype=torch.float32)
static_feat_f = torch.from_numpy(static_feat_f).to(device, dtype=torch.float32)

# è¯­ä¹‰è¾¹
poi_adj_c = torch.from_numpy(poi_adj_c).to(device, dtype=torch.float32)
poi_adj_f = torch.from_numpy(poi_adj_f).to(device, dtype=torch.float32)
risk_adj_c = torch.from_numpy(risk_adj_c).to(device, dtype=torch.float32)
risk_adj_f = torch.from_numpy(risk_adj_f).to(device, dtype=torch.float32)
road_adj_c = torch.from_numpy(road_adj_c).to(device, dtype=torch.float32)
road_adj_f = torch.from_numpy(road_adj_f).to(device, dtype=torch.float32)

# ç”Ÿæˆç½‘æ ¼æ©ç 
valid_mask_c = torch.sum(grid_node_map_c, dim=1).view(int(np.sqrt(grid_node_map_c.shape[0])), -1).to(device,
                                                                                                     dtype=torch.float32)
valid_mask_f = torch.sum(grid_node_map_f, dim=1).view(int(np.sqrt(grid_node_map_f.shape[0])), -1).to(device,
                                                                                                     dtype=torch.float32)

adj_matrices_c = torch.stack([poi_adj_c, risk_adj_c, road_adj_c], dim=0).to(device, dtype=torch.float32)
adj_matrices_f = torch.stack([poi_adj_f, risk_adj_f, road_adj_f], dim=0).to(device, dtype=torch.float32)

model = ST_ViT().to(device)


def print_params_count(model):
    total_params = sum(p.numel() for p in model.parameters())  # è®¡ç®—æ€»å‚æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # å¯è®­ç»ƒå‚æ•°

    # æ ¼å¼åŒ–è¾“å‡º
    print(f"Total Parameters: {total_params:,} ({total_params / 1e6:.2f}M)")
    print(f"Trainable Parameters: {trainable_params:,}")


print_params_count(model)

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

model_name = model.__class__.__name__
logging.info(f"Model Name: {model_name}\n")


def weighted_mse_loss(pred, target, alpha=1.0):
    """
    åŠ æƒMSEæŸå¤±å‡½æ•°
    Args:
        pred: é¢„æµ‹å€¼ï¼Œå½¢çŠ¶ä¸º (S, label, node)
        target: ç›®æ ‡å€¼ï¼ˆä»…åŒ…å«0å’Œ1ï¼‰ï¼Œå½¢çŠ¶ä¸predç›¸åŒ
        alpha: æ ‡ç­¾1çš„æƒé‡å€æ•°
    """
    # è®¡ç®—å¹³æ–¹å·®
    squared_error = (pred - target) ** 2

    # ç”Ÿæˆæƒé‡çŸ©é˜µï¼šæ ‡ç­¾1çš„ä½ç½®æƒé‡ä¸ºalphaï¼Œå¦åˆ™ä¸º1
    weights = torch.where(target == 1, alpha, 1.0)

    # è®¡ç®—åŠ æƒæŸå¤±å¹¶å–å‡å€¼
    weighted_loss = (squared_error * weights).mean()

    return weighted_loss


criterion_c = nn.MSELoss()

optimizer_c = optim.Adam(model.c_parameters(), lr=0.001)
scheduler_c = StepLR(optimizer_c, step_size=1, gamma=0.95)  # æ¯1ä¸ªepochè¡°å‡5%

# checkpointname="saved_models/20250324_121517/STGNN_DualBranch_epoch_22.pth"
# checkpoint = torch.load(checkpointname)  # æ›¿æ¢æˆå®é™…è·¯å¾„
# model.load_state_dict(checkpoint['model_state_dict'])
# optimizer_f.load_state_dict(checkpoint['optimizer_f_state_dict'])
# optimizer_c.load_state_dict(checkpoint['optimizer_c_state_dict'])
# logging.info(f"æˆåŠŸåŠ è½½: {checkpointname}\n")


from tqdm import tqdm
import time

for epoch in range(num_epochs):
    model.train()
    t1 = time.time()
    train_loss_c = 0.0

    # tqdm è¿›åº¦æ¡å°è£…è®­ç»ƒé›†
    progress_bar = tqdm(train_loader, desc=f"Epoch [{epoch + 1}/{num_epochs}] Training", leave=False)
    for batch in progress_bar:
        X_c, y_c, X_f, y_f, node_X_c, node_X_f, target_time_feature = batch
        X_c, y_c, target_time_feature = X_c.to(device), y_c.to(device), target_time_feature.to(device)

        # å‰å‘ä¼ æ’­
        c_pred = model(X_c)

        # ç½‘æ ¼ -> èŠ‚ç‚¹æ˜ å°„
        c_pred_mapped = torch.matmul(c_pred.view(c_pred.shape[0], c_pred.shape[1], -1), grid_node_map_c)
        y_c_mapped = torch.matmul(y_c.view(y_c.shape[0], y_c.shape[1], -1), grid_node_map_c)

        # æŸå¤±è®¡ç®—ä¸åå‘ä¼ æ’­
        loss_c = weighted_mse_loss(c_pred_mapped, y_c_mapped)
        optimizer_c.zero_grad()
        loss_c.backward()
        optimizer_c.step()

        train_loss_c += loss_c.item()
        progress_bar.set_postfix({"TrainLoss": f"{loss_c.item():.4f}"})

    scheduler_c.step()
    t2 = time.time()

    # ================= éªŒè¯é˜¶æ®µ =================
    model.eval()
    val_loss_c = 0.0
    with torch.no_grad():
        for batch in val_loader:
            X_c, y_c, X_f, y_f, node_X_c, node_X_f, target_time_feature = batch
            X_c, y_c, target_time_feature = X_c.to(device), y_c.to(device), target_time_feature.to(device)

            c_pred = model(X_c)
            c_pred_mapped = torch.matmul(c_pred.view(c_pred.shape[0], c_pred.shape[1], -1), grid_node_map_c)
            y_c_mapped = torch.matmul(y_c.view(y_c.shape[0], y_c.shape[1], -1), grid_node_map_c)
            val_loss_c += criterion_c(c_pred_mapped, y_c_mapped).item()

    val_loss_c /= len(val_loader)
    train_loss_c /= len(train_loader)
    t3 = time.time()

    # ================= æµ‹è¯•é˜¶æ®µ =================
    model.eval()
    with torch.no_grad():
        all_c_preds, all_y_c = [], []
        for batch in test_loader:
            X_c, y_c, X_f, y_f, node_X_c, node_X_f, target_time_feature = batch
            X_c, y_c = X_c.to(device), y_c.to(device)
            c_pred = model(X_c)

            c_pred_flat = c_pred.view(c_pred.shape[0] * c_pred.shape[1], -1)
            y_c_flat = y_c.view(y_c.shape[0] * y_c.shape[1], -1)

            c_pred_mapped = torch.matmul(c_pred_flat, grid_node_map_c)
            y_c_mapped = torch.matmul(y_c_flat, grid_node_map_c)

            all_c_preds.append(c_pred_mapped)
            all_y_c.append(y_c_mapped)

        c_preds = torch.cat(all_c_preds, dim=0)
        y_c = torch.cat(all_y_c, dim=0)

        # å„ç±»æŒ‡æ ‡
        MSE_C = calculate_rmse(c_preds, y_c)
        MAP_C = calculate_dynamic_map(c_preds, y_c)
        ACC_CL = calculate_acc_at_dynamic_L(c_preds, y_c)
        metrics_C = calculate_metrics(c_preds, y_c)
        F1_C, ACC_C, AUC_PR_C, AUC_ROC_C = metrics_C["F1 Score"], metrics_C["Accuracy"], metrics_C["AUC-PR"], metrics_C[
            "AUC-ROC"]
        Precision_C, Recall_C = metrics_C["Precision"], metrics_C["Recall"]

    t4 = time.time()

    # ================= è¾“å‡ºæ—¥å¿— =================
    logging.info(f"Epoch [{epoch + 1}/{num_epochs}]")
    logging.info(f"Time: Train={t2 - t1:.1f}s | Val/Test={t4 - t2:.1f}s")
    logging.info(f"Train Loss: {train_loss_c:.4f} | Val Loss: {val_loss_c:.4f}")
    logging.info(f"RMSE: {MSE_C:.4f} | MAP: {MAP_C:.4f} | Acc@L: {ACC_CL:.4f}")
    logging.info(f"F1: {F1_C:.4f} | Acc: {ACC_C:.4f} | Prec: {Precision_C:.4f} | Rec: {Recall_C:.4f}")
    logging.info(f"AUC-PR: {AUC_PR_C:.4f} | AUC-ROC: {AUC_ROC_C:.4f}\n")

    # tqdm æ§åˆ¶å°å®æ—¶è¾“å‡º
    print(f"\nğŸ“˜ Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss_c:.4f} | Val Loss: {val_loss_c:.4f}")
    print(f"   RMSE={MSE_C:.4f}, MAP={MAP_C:.4f}, F1={F1_C:.4f}, AUC-ROC={AUC_ROC_C:.4f}, Acc@L={ACC_CL:.4f}")

    # ================= æ¨¡å‹ä¿å­˜ =================
    save_dir = os.path.join("saved_models", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    checkpoint_path = os.path.join(save_dir, f"{model_name}_epoch_{epoch + 1}.pth")

    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_c_state_dict': optimizer_c.state_dict(),
        'scheduler_c_state_dict': scheduler_c.state_dict(),
        'loss': train_loss_c
    }, checkpoint_path)

    print(f"ğŸ’¾ Saved model checkpoint: {checkpoint_path}")


